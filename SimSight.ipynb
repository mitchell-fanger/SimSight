{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer: 3054\n",
      "computation: 92\n",
      "computational: 47\n",
      "computing: 125\n",
      "compute: 92\n",
      "computed: 1023\n",
      "\"mathematical model \": 8\n",
      "\"mathematical modeling \": 0\n",
      "\"mathematical models\": 0\n",
      "\"mathematical modeled\": 0\n",
      "\"math model\": 7\n",
      "\"math modeling\": 0\n",
      "\"math modeled\": 0\n",
      "\"math models\": 0\n",
      "\"mathematics model\": 0\n",
      "\"mathematics models\": 0\n",
      "\"mathematics modeling\": 0\n",
      "\"mathematics modeled\": 0\n",
      "\"physics modeling\": 3\n",
      "\"physics modeled\": 0\n",
      "\"finite element\": 156\n",
      "\"finite element analysis\": 133\n",
      "FEA: 202\n",
      "\"finite element model\": 4\n",
      "\"finite element modeling\": 4\n",
      "\"finite element method\": 2\n",
      "\"finite element models\": 0\n",
      "\"finted element modeled\": 0\n",
      "\"finite element analyzed\": 0\n",
      "\"numerical analysis\": 1\n",
      "\"numeric analysis\": 0\n",
      "\"numerically analyzed\": 0\n",
      "CFD: 9\n",
      "\"computational fluid dynamics\": 1\n",
      "\"computer fluid dynamics\": 0\n",
      "\"Computed fluid dynamics\": 0\n",
      "\"computational fluid dynamic\": 0\n",
      "\"computer fluid dynamic\": 0\n",
      "\"compute fluid dynamic\": 0\n",
      "\"computing fluid dynamic\": 0\n",
      "\"computation fluid dynamic\": 0\n",
      "\"MRI model\": 0\n",
      "\"electromagnetic model\": 0\n",
      "\"electromagnetic models\": 0\n",
      "\"electromagnetic modeling\": 0\n",
      "\"electromagnetic modeled\": 0\n",
      "\"computed electromagnetic model\": 0\n",
      "\"MRI modeled\": 0\n",
      "\"magnetic resonance imaging model\": 0\n",
      "\"magnetic resonance imaging models\": 0\n",
      "\"magnetic resonance imaging modeling\": 0\n",
      "\"magnetic resonance imaging modeled\": 0\n",
      "\"optics model\": 0\n",
      "\"optics models\": 0\n",
      "\"optics modeled\": 0\n",
      "\"optic model\": 0\n",
      "\"optic models\": 0\n",
      "\"optic modeling\": 0\n",
      "modeled: 29\n",
      "\"computed optics\": 0\n",
      "\"computed optic\": 0\n",
      "\"thermal modeling\": 1\n",
      "\"thermal model\": 0\n",
      "\"thermal modeled\": 0\n",
      "\"thermal models\": 0\n",
      "\"computed acoustics\": 0\n",
      "\"computed acoustic\": 0\n",
      "\"computing acoustics\": 0\n",
      "\"computing acoustic\": 0\n",
      "\"acoustic model\": 0\n",
      "acoustic: 1114\n",
      "\"thermal model\": 0\n",
      "\"thermal models\": 0\n",
      "\"thermal modeling\": 1\n",
      "\"thermal modeled\": 0\n",
      "\"Virtual Population\": 1\n",
      "virtualization: 5\n",
      "virtualized: 10\n",
      "virtualizations: 0\n",
      "\"virtual platform\": 4\n",
      "VF: 66\n",
      "\"virutal patients\": 0\n",
      "\"Virtual reality\": 7\n",
      "VR: 221\n",
      "\"software as medical device\": 0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from abc import ABC\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "from whoosh import index\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "import sys\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "from whoosh.index import open_dir\n",
    "from whoosh.analysis import StandardAnalyzer\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#abstract class (do not instantiate)\n",
    "class Searchable:\n",
    "    \n",
    "    def __init__(self, root, index):\n",
    "        \n",
    "        self.root = root\n",
    "        self.index = index\n",
    "    \n",
    "    #provides a full text search of all pdfs over the root\n",
    "    def fullTextSearch(self, terms, reindex):\n",
    "        \n",
    "        if(reindex):\n",
    "            self.createSearchableData()\n",
    "        \n",
    "        self.phrases = terms\n",
    "        r = []\n",
    "    \n",
    "        for i in range(len(self.phrases)):\n",
    "            r.append(self.termQuery(self.phrases[i], self.index))\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    \n",
    "    #helper, creates a searchable index\n",
    "    def createSearchableData(self):  \n",
    "        \n",
    "        schema = Schema(title=TEXT(stored=True),path=ID(stored=True), content=TEXT(phrase=True, analyzer=StandardAnalyzer(stoplist=None)))\n",
    "        if not os.path.exists(\"indexdir\"):\n",
    "            os.mkdir(\"indexdir\")\n",
    "\n",
    "        ix = index.create_in(\"indexdir\", schema, indexname = self.index)\n",
    "        writer = ix.writer()\n",
    "\n",
    "        filepaths = [os.path.join(self.root,i) for i in os.listdir(self.root)]\n",
    "        for path in filepaths:\n",
    "            try:\n",
    "                text = self.getPDFText(path)\n",
    "                text2 = self.getPDFText2(path)\n",
    "                for i in range(len(text)):\n",
    "                    writer.add_document(title=path.split(\"\\\\\")[1] + '_Page_' + str(i), path=path,\\\n",
    "                        content=text[i])\n",
    "                    writer.add_document(title=path.split(\"\\\\\")[1] + '_Page_' + str(i) + '_2', path=path,\\\n",
    "                        content=text2[i])\n",
    "                #print(path.split(\"\\\\\")[1] + ' has been indexed')\n",
    "                \n",
    "            except:\n",
    "                print('error in ' + path)\n",
    "            \n",
    "            \n",
    "        writer.commit()            \n",
    "\n",
    "        \n",
    "    \n",
    "    #helper, searches index for particular phrase and prints results\n",
    "    def termQuery(self, phrase, index):\n",
    "        \n",
    "        ix = open_dir(\"indexdir\", indexname = index)\n",
    " \n",
    "        qp = QueryParser(\"content\", schema=ix.schema)\n",
    "        q = qp.parse(phrase)\n",
    "    \n",
    "        res = []\n",
    "    \n",
    "        with ix.searcher() as s:\n",
    "            results = s.search(q, limit=None)\n",
    "            for result in results:\n",
    "                res.append(str(result))\n",
    "            return res\n",
    "        \n",
    "    #static helper, returns text found in pdf\n",
    "    def getPDFText(self, path):\n",
    "        text = []\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            pdf = PdfFileReader(f)\n",
    "            numPages = pdf.getNumPages()\n",
    "            for pageNum in range(numPages):\n",
    "                page = pdf.getPage(pageNum)\n",
    "                \n",
    "                #reads in specific page of doc\n",
    "                try:\n",
    "                    text.append(page.extractText().replace('\\n', ' '))\n",
    "                except TypeError:\n",
    "                    print('TypeError in ' + path)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    #replaces line breaks with blanks instead of spaces\n",
    "    def getPDFText2(self, path):\n",
    "        text = []\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            pdf = PdfFileReader(f)\n",
    "            numPages = pdf.getNumPages()\n",
    "            for pageNum in range(numPages):\n",
    "                page = pdf.getPage(pageNum)\n",
    "                \n",
    "                #reads in specific page of doc\n",
    "                try:\n",
    "                    text.append(page.extractText().replace('\\n', ''))\n",
    "                except TypeError:\n",
    "                    print('error in ' + path)\n",
    "\n",
    "        return text\n",
    "\n",
    "    #static, call to compile pdf with array of results\n",
    "    def compileResults(self, path, phrases, results):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        for i in range(len(phrases)):\n",
    "            output = PdfFileWriter()\n",
    "        \n",
    "            docs = set([])\n",
    "            \n",
    "            #go through all results, 3 = path, 7 = pdf and page num\n",
    "            for result in results[i]:\n",
    "                data = result.split('\\'')\n",
    "                \n",
    "                pdfName = data[3]\n",
    "                pageNum = data[7][len(data[3][11:]) + 6:]\n",
    "                \n",
    "                if '_' in pageNum:\n",
    "                    pageNum = pageNum[0: len(pageNum)-2]\n",
    "                \n",
    "                pdf = pdfName + pageNum\n",
    "                \n",
    "                length = len(docs)\n",
    "                docs.add(pdf)\n",
    "                if length != len(docs):\n",
    "                   \n",
    "                    with open(data[3], 'rb') as f:\n",
    "                        pdf = PdfFileReader(f)              \n",
    "                        output.addPage(pdf.getPage(int(pageNum)))\n",
    "                        \n",
    "                        name = phrases[i].replace('\"', '')\n",
    "\n",
    "                        with open(path +'\\\\' + name + '.pdf', 'wb') as outputStream:\n",
    "                            output.write(outputStream)\n",
    "    \n",
    "    #prints raw results                \n",
    "    def printResults(self, phrases, results):\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "            print(phrases[i] + ':')\n",
    "        \n",
    "            for element in results[i]:\n",
    "                print(element)\n",
    "                print(element[21:29])\n",
    "                \n",
    "        #returns a set of all unique PMAs\n",
    "    def getSets(self, results):\n",
    "        \n",
    "        toRet = []\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "\n",
    "            unique = set([])\n",
    "\n",
    "            for element in results[i]:\n",
    "                unique.add(element.split('\\'')[3].split('\\\\\\\\')[1].split('.')[0].split('B')[0])\n",
    "\n",
    "            toRet.append(unique)\n",
    "        \n",
    "        return toRet\n",
    "    \n",
    "\n",
    "\n",
    "#searches through pma database; can call fullTextSearch and deleteFiles\n",
    "class PSearch(Searchable):\n",
    "    \n",
    "    def __init__(self, r, i):\n",
    "        super().__init__(r, i)\n",
    "        self.committee = ''\n",
    "       \n",
    "        if not os.path.exists(self.root):\n",
    "            os.mkdir(self.root)\n",
    "        \n",
    "        \n",
    "    #download SSEDs from a start date to an end date (downloads to root directory)\n",
    "    def downloadSSEDs(s, e):\n",
    "        \n",
    "        print('hello1')\n",
    "        \n",
    "        for i in range(s.monthsInBetween(e)):\n",
    "            self.getSummaries(self.getPMAS(s.getDate(), s.lastDay()))\n",
    "            s.nextMonth()\n",
    "        \n",
    "        self.getSummaries(self.getPMAS(e.firstDay(), e.getDate()))\n",
    "    \n",
    "    #deletes all files that were downloaded (use after performing full text searches)    \n",
    "    def deleteFiles(self):\n",
    "        filepaths = [os.path.join(self.root,i) for i in os.listdir(self.root)]\n",
    "        for path in filepaths:\n",
    "            os.remove(path)\n",
    "        \n",
    "    #helper, returns url of pma summary pdf and determines if pma has summary\n",
    "    def getSummary(self, URL):\n",
    "\n",
    "        page_response = requests.get(URL, timeout=5)\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "        textContent = page_content.find_all(style=\"text-decoration:underline;\")\n",
    "\n",
    "        #checks if original pma contains a summary\n",
    "        for element in textContent:\n",
    "            if(element.text == 'Summary of Safety and Effectiveness'):\n",
    "                \n",
    "                \n",
    "                if int(URL[69:71]) <= 1:\n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + '/' + URL[68:75] + 'B.pdf'\n",
    "                \n",
    "                if int(URL[69:71]) >= 70:\n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + '/' + URL[68:75] + 'B.pdf'\n",
    "                \n",
    "                #if the pma version is less than 10 the url is formatted a little different\n",
    "                elif int(URL[69:71]) < 10:\n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + URL[70:71] + '/' + URL[68:75] + 'B.pdf'\n",
    "                \n",
    "                return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + URL[69:71] + '/' + URL[68:75] + 'B.pdf'\n",
    "\n",
    "        return URL[68:75] + ' does not contain a Summary of Safety and Effectiveness' \n",
    "\n",
    "    #helper, gets the url of pmas given a range of time\n",
    "    def getPMAS(self, f, t):\n",
    "        return 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMA/pma.cfm?start_search=1&applicant=&tradename=&productcode=&pmanumber=&supplementnumber=&advisorycommittee=' + self.committee + '&docketnumber=&supplementtype=&expeditedreview=&center=D&ivdproducts=off&combinationproducts=off&decisiondatefrom=' + formatDate(f) + '&decisiondateto=' + formatDate(t) +'&noticedatefrom=&noticedateto=&znumber=&pagenum=500'\n",
    "    \n",
    "    #helper, downloads all pma summary pdfs in the url\n",
    "    def getSummaries(self, url):\n",
    "\n",
    "        startUrl = 'https://www.accessdata.fda.gov'\n",
    "\n",
    "        page_response = requests.get(url, timeout=5)\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "        textContent = page_content.find_all(style=\"text-decoration:underline;\")\n",
    "\n",
    "        #looks through all pmas in url\n",
    "        for element in textContent:\n",
    "            string = str(element)\n",
    "            deviceURL = startUrl + string[9: 9 + string[9:len(string)].index('\\\"')]\n",
    "\n",
    "            #if the pma is original\n",
    "            if len(deviceURL) == 75:\n",
    "\n",
    "                file_url = self.getSummary(deviceURL)\n",
    "                \n",
    "                #if the pma does not have a summary\n",
    "                if(len(file_url) == 62):\n",
    "                    print(file_url)\n",
    "                else:\n",
    "                    r = requests.get(file_url, stream = True) \n",
    "                    \n",
    "                    start = len(file_url) - 12\n",
    "                    \n",
    "                    #download pmas into Summaries folder\n",
    "                    with open(self.root + '/' + file_url[start:59],\"wb\") as pdf: \n",
    "                        for chunk in r.iter_content(chunk_size=1024): \n",
    "                            if chunk: \n",
    "                                pdf.write(chunk)\n",
    "    \n",
    "    #returns total number of unique PMAs                            \n",
    "    def total(self, results):\n",
    "\n",
    "        unique = set([])\n",
    "\n",
    "        for i in range(len(results)):\n",
    "\n",
    "            for element in results[i]:\n",
    "                unique.add(element.split('\\'')[3])\n",
    "\n",
    "        return str(len(unique))\n",
    "    \n",
    "    #prints number of unique PMAs per keyword\n",
    "    def resultValues(self, phrases, results):\n",
    "         \n",
    "        for i in range(len(results)):\n",
    "            \n",
    "            unique = set([])\n",
    "            \n",
    "            for element in results[i]:\n",
    "                unique.add(element.split('\\'')[3])\n",
    "                \n",
    "            print(phrases[i] + ': ' + str(len(unique)))\n",
    "    \n",
    "\n",
    "    \n",
    "    #writes csvs containing information about PMAs found in each search term\n",
    "    def writeSpreadsheets(self, root, phrases, results):\n",
    "        \n",
    "        res = self.getSets(results)\n",
    "        \n",
    "        for i in range(len(res)):\n",
    "            \n",
    "            name = phrases[i].replace('\"', '')\n",
    "            if len(res[i]) != 0:\n",
    "                pmaInfoList(root, name, res[i])\n",
    "    \n",
    "    #writes a csv containing information about PMAs found in entire search\n",
    "    def getTotalSpreadsheet(self, root, results):\n",
    "        \n",
    "        unique = set([])\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "            \n",
    "            for element in results[i]:\n",
    "                unique.add(element.split('\\'')[3].split('\\\\\\\\')[1].split('.')[0].split('B')[0])\n",
    "        \n",
    "        if len(unique) != 0:\n",
    "            pmaInfoList(root, 'total', unique)\n",
    "    \n",
    "    #plots a simple graph separated by decision date\n",
    "    def plotTotal(self, root):\n",
    "        with open(root + '\\\\total.csv') as file:\n",
    "            string = file.read().replace('\\n\\n','\\n')\n",
    "            lines = string.splitlines()\n",
    "    \n",
    "        values = [0] * 18\n",
    "\n",
    "        for line in lines:\n",
    "            values[int(line[14:18])-2002] += 1\n",
    "        plt.bar(range(0,18), values)\n",
    "        #plt.axis([0,17,0,30])\n",
    "        plt.show()\n",
    "    \n",
    "    #plots a graph based on decision date and advisory committee\n",
    "    def plotTotalTwo(self, root, word):\n",
    "\n",
    "        with open('advisorycommittees.txt', 'r') as file:\n",
    "            advisoryCommittees = file.read().splitlines()\n",
    "\n",
    "        with open(root + '\\\\total.csv') as file:\n",
    "            string = file.read().replace('\\n\\n','\\n')\n",
    "            lines = string.splitlines()\n",
    "\n",
    "        arrays = {}\n",
    "\n",
    "        for ac in advisoryCommittees:\n",
    "            arrays[ac] = [0] * 18\n",
    "\n",
    "        for line in lines:\n",
    "            arrays[line.split(',')[2]][int(line[14:18])-2002] += 1\n",
    "\n",
    "        ind = np.arange(18)    # the x locations for the groups\n",
    "        width = .35     # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "        p = []\n",
    "        committees = getKeys(arrays)\n",
    "        p.append(plt.bar(ind, arrays[committees[0]], width))\n",
    "        p.append(plt.bar(ind, arrays[committees[1]], width, bottom = arrays[committees[0]]))\n",
    "        bars = np.add(arrays[committees[0]], [0] * 18).tolist()\n",
    "\n",
    "        for i in range(2, len(committees)):\n",
    "            bars = np.add(arrays[committees[i-1]], bars).tolist()\n",
    "            p.append(plt.bar(ind, arrays[committees[i]], width, bottom = bars))\n",
    "\n",
    "        pLegend = []\n",
    "        cLegend = []\n",
    "\n",
    "        for ps in p:\n",
    "            pLegend.append(ps[0])\n",
    "\n",
    "        for c in committees:\n",
    "            cLegend.append(c)\n",
    "\n",
    "        plt.ylabel('Number of Unique SSEDs')\n",
    "        plt.xlabel('Year')\n",
    "        plt.title('Number of Unique SSEDs that contain: ' + word)\n",
    "        plt.xticks(ind, range(2002, 2020), rotation='vertical')\n",
    "        plt.legend(pLegend, cLegend, bbox_to_anchor=(1,1.05))\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    #returns all PMAs found in first results but not second results\n",
    "    def subsearch(self, first, second):\n",
    "        \n",
    "        set1 = set([])\n",
    "        set2 = set([])\n",
    "        \n",
    "        for result in first:\n",
    "            for element in result: \n",
    "                set1.add(element.split('\\'')[3].split('\\\\\\\\')[1])\n",
    "                \n",
    "        for result in second:\n",
    "            for element in result: \n",
    "                set2.add(element.split('\\'')[3].split('\\\\\\\\')[1])\n",
    "        \n",
    "        toReturn = set([])\n",
    "        \n",
    "        for element in set1:\n",
    "            if not element in set2:\n",
    "                toReturn.add(element)\n",
    "                \n",
    "        return toReturn\n",
    "\n",
    "#returns list of keys given dictionary\n",
    "def getKeys(dictionary):\n",
    "\n",
    "    l = []\n",
    "\n",
    "    for k,v in dictionary.items():\n",
    "        l.append(k)\n",
    "\n",
    "    return l\n",
    "\n",
    "#searches public 510K database (only words from 2004 - present, anything past 2004 must be OCR'd)\n",
    "class KSearch(Searchable):\n",
    "        \n",
    "    def __init__(self, r, i):\n",
    "        \n",
    "        super().__init__(r,i)\n",
    "\n",
    "        if not os.path.exists(self.root):\n",
    "            os.mkdir(self.root)\n",
    "    \n",
    "    #downloads summaries from range of dates\n",
    "    def downloadSums(self, s, e):\n",
    "        for i in range(s.monthsInBetween(e)):\n",
    "            self.loadDatabase(s.getDate(), s.lastDay())\n",
    "            s.nextMonth()\n",
    "\n",
    "        self.loadDatabase(e.firstDay(), e.getDate())\n",
    "            \n",
    "    def getSumURL(self, kURL):\n",
    "        page_response = requests.get(kURL, timeout=5)\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "        textContent = page_content.find_all(style = 'text-decoration:underline;')\n",
    "        for i in range(len(textContent)):\n",
    "\n",
    "            if ('summary' in str(textContent[i])):\n",
    "                ID = kURL.split('ID=')[1]\n",
    "\n",
    "                year = int(ID[1:3])\n",
    "\n",
    "                if year > 9:\n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + ID[1:3] + '/'+ ID + '.pdf'\n",
    "                elif year > 1:\n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf' + ID[2:3] + '/'+ ID + '.pdf'\n",
    "                else: \n",
    "                    return 'http://www.accessdata.fda.gov/cdrh_docs/pdf/'+ ID + '.pdf'\n",
    "        return 'no summary found'\n",
    "\n",
    "\n",
    "    #given the URL of a 510K summary, download it as a PDF\n",
    "    def download510K(self, sURL):\n",
    "        r = requests.get(sURL, stream = True) \n",
    "\n",
    "        #download pmas into Summaries folder\n",
    "        with open(self.root + '/' + sURL.split('/')[5],\"wb\") as pdf: \n",
    "            for chunk in r.iter_content(chunk_size=1024): \n",
    "                if chunk: \n",
    "                    pdf.write(chunk)\n",
    "    \n",
    "    #if the webpage times out attempt it again\n",
    "    def catchTimeout(self, detailsURL):\n",
    "        try:\n",
    "            sumURL = self.getSumURL(detailsURL)\n",
    "\n",
    "            if sumURL != 'no summary found':\n",
    "                self.download510K(sumURL)\n",
    "                print('Successfully downloaded ' + detailsURL)\n",
    "                \n",
    "        except:\n",
    "            print('ReadTimeout: ' + detailsURL)\n",
    "            self.catchTimeout(detailsURL)\n",
    "    \n",
    "    def loadDatabase(self, start, end):\n",
    "        url = 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?start_search=1&Center=&Panel=&ProductCode=&KNumber=&Applicant=&DeviceName=&Type=&ThirdPartyReviewed=&ClinicalTrials=&Decision=&DecisionDateFrom=' + start+'&DecisionDateTo='+ end+'&IVDProducts=&Redact510K=&CombinationProducts=&ZNumber=&PAGENUM=500'\n",
    "        page_response = requests.get(url, timeout=5)\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "        textContent = page_content.find_all(style = 'text-decoration:underline;')\n",
    "\n",
    "        for i in range(2, len(textContent)):\n",
    "            if i%2 == 0:\n",
    "                detailsURL = 'https://www.accessdata.fda.gov' + str(textContent[i]).split('\"')[1]\n",
    "                \n",
    "                self.catchTimeout(detailsURL)\n",
    "    \n",
    "    def resultValues(self, phrases, results):\n",
    "\n",
    "        for i in range(len(results)):\n",
    "\n",
    "            unique = set([])\n",
    "\n",
    "            for element in results[i]:\n",
    "                unique.add(element.split('\\'')[3])\n",
    "\n",
    "            print(phrases[i] + ': ' + str(len(unique)))\n",
    "                    \n",
    "        #writes csvs containing information about PMAs found in each search term\n",
    "    def writeSpreadsheets(self, root, phrases, results):\n",
    "        \n",
    "        res = self.getSets(results)\n",
    "        \n",
    "        for i in range(len(res)):\n",
    "            \n",
    "            name = phrases[i].replace('\"', '')\n",
    "            if len(res[i]) != 0:\n",
    "                kInfoList(root, name, res[i])\n",
    "                \n",
    "    #returns all 510Ks found in first results but not second results\n",
    "    def subsearch(self, first, second):\n",
    "        \n",
    "        set1 = set([])\n",
    "        set2 = set([])\n",
    "        \n",
    "        for result in first:\n",
    "            for element in result: \n",
    "                set1.add(element.split('\\'')[3].split('\\\\\\\\')[1])\n",
    "                \n",
    "        for result in second:\n",
    "            for element in result: \n",
    "                set2.add(element.split('\\'')[3].split('\\\\\\\\')[1])\n",
    "        \n",
    "        toReturn = set([])\n",
    "        \n",
    "        for element in set1:\n",
    "            if not element in set2:\n",
    "                toReturn.add(element)\n",
    "                \n",
    "        return toReturn\n",
    "                                \n",
    "#creates an object to be searched from already existing pdfs; can call fullTextSearch                                \n",
    "class RootSearch(Searchable):\n",
    "    \n",
    "    def __init__(self, r, i):\n",
    "        super().__init__(r, i)\n",
    "                                \n",
    "#date helper class\n",
    "class Date:\n",
    "    \n",
    "    daysInMonth = [[0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]]\n",
    "    \n",
    "    \n",
    "    def __init__(self, month, day, year):\n",
    "        self.m = month\n",
    "        self.d = day\n",
    "        self.y = year\n",
    "    \n",
    "    def getDate(self):\n",
    "        \n",
    "        if(len(str(self.m)) > 1):\n",
    "            toReturn = str(self.m)\n",
    "        else:\n",
    "            toReturn = '0' + str(self.m)\n",
    "            \n",
    "        if(len(str(self.d)) > 1):\n",
    "            toReturn += '/' + str(self.d)\n",
    "        else:\n",
    "            toReturn += '/0' + str(self.d)\n",
    "            \n",
    "        toReturn += '/' + str(self.y)\n",
    "        \n",
    "        return toReturn\n",
    "    def nextMonth(self):\n",
    "        if(self.m == 12):\n",
    "            self.m = 1\n",
    "            self.y += 1\n",
    "        else:\n",
    "            self.m +=1\n",
    "        self.d = 1\n",
    "            \n",
    "    def prevMonth(self):\n",
    "        if(self.m == 1):\n",
    "            self.m = 12\n",
    "            self.y -= 1\n",
    "        else:\n",
    "            self.m -= 1\n",
    "        self.d = 1\n",
    "    \n",
    "    def lastDay(self):\n",
    "        \n",
    "        if self.m < 10:\n",
    "            toReturn = '0' + str(self.m)\n",
    "        else:\n",
    "            toReturn = str(self.m)\n",
    "        \n",
    "        return toReturn + '/' + str(self.daysInMonth[self.y%4][self.m]) + '/' + str(self.y)\n",
    "    \n",
    "    def firstDay(self):\n",
    "          \n",
    "        if self.m < 10:\n",
    "            toReturn = '0' + str(self.m)\n",
    "        else:\n",
    "            toReturn = str(self.m)\n",
    "        \n",
    "        return toReturn + '/01/' + str(self.y)\n",
    "    \n",
    "    def monthsInBetween(self, other):\n",
    "        return (other.y - self.y)*12 + (other.m - self.m)\n",
    "    \n",
    "#formats date so it can be sent into a request as 01%2F01%2F2017 \n",
    "def formatDate(date):\n",
    "    return date[0:2] + '%2F' +date[3:5] + '%2F' + date[6:]\n",
    "        \n",
    "#given text file, reads in terms to be searched    \n",
    "def readInTerms(file):\n",
    "    with open(file, 'r', encoding = 'utf-8') as f:\n",
    "        words = f.read()\n",
    "\n",
    "    wordList = words.splitlines()\n",
    "    \n",
    "    toRet = []\n",
    "    \n",
    "    for word in wordList:\n",
    "        if ' ' in word:\n",
    "            toRet.append('\"' + word + '\"')\n",
    "        else:\n",
    "            toRet.append(word)\n",
    "            \n",
    "    return toRet\n",
    "\n",
    "#pulls info from public PMA database given certain \n",
    "def getInfo(ID, lines):\n",
    "    url = 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpma/pma.cfm?id=' + ID\n",
    "    page_response = requests.get(url, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "    first = 'none'\n",
    "    second = 'none'\n",
    "    \n",
    "    textContent = page_content.find_all('tr')\n",
    "    \n",
    "    for i in range(9, len(textContent)):\n",
    "        if 'Decision Date' in str(textContent[i]):\n",
    "            first = str(textContent[i]).split('<td align=\"Left\">')[1][0:10]\n",
    "        elif 'Advisory Committee' in str(textContent[i]):\n",
    "            second = str(textContent[i]).split('<td align=\"Left\">')[1].split('</td>')[0]\n",
    "    lines.append([ID, first, second])\n",
    "    return lines\n",
    "\n",
    "def getKInfo(ID, lines):\n",
    "    url = 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=' + ID\n",
    "    page_response = requests.get(url, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "    first = 'none'\n",
    "    second = 'none'\n",
    "    \n",
    "    textContent = page_content.find_all('tr')\n",
    "    \n",
    "    for i in range(9, len(textContent)):\n",
    "        if 'Decision Date' in str(textContent[i]):\n",
    "            first = str(textContent[i]).split('<td align=\"left\">')[1][1:11]\n",
    "        elif '510k Review Panel' in str(textContent[i]):\n",
    "            second = str(textContent[i]).split('<td align=\"left\">')[1].split('</td>')[0]\n",
    "    lines.append([ID, first, second.strip()])\n",
    "    return lines\n",
    "\n",
    "def writeToCSV(root, lines, file):\n",
    "    \n",
    "    with open(root + '\\\\' + file + '.csv', 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile)\n",
    "        filewriter.writerows(lines)\n",
    "\n",
    "def pmaInfoDir(path):\n",
    "    filepaths = [os.path.join(path,i) for i in os.listdir(path)]\n",
    "    lines = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        lines = getInfo(file.split('\\\\')[1][0:7], lines)\n",
    "    \n",
    "    writeToCSV(lines)\n",
    "    \n",
    "def pmaInfoList(root, phrase, names):\n",
    "    lines = []\n",
    "    \n",
    "    for name in names:\n",
    "        lines = getInfo(name, lines)\n",
    "        \n",
    "    writeToCSV(root, lines, phrase)\n",
    "    \n",
    "def kInfoList(root, phrase, names):\n",
    "    lines = []\n",
    "    \n",
    "    for name in names:\n",
    "        lines = getKInfo(name, lines)\n",
    "        \n",
    "    writeToCSV(root, lines, phrase)\n",
    "\n",
    "search = KSearch('KSummaries', 'KSums')\n",
    "terms = readInTerms('keywordSearch.txt')\n",
    "#terms = ['finite', 'element']\n",
    "#search.downloadSums(Date(1,1,2012), Date(1,1,2013))\n",
    "result = search.fullTextSearch(terms, False)\n",
    "search.resultValues(terms, result)\n",
    "#search.printResults(terms, result)\n",
    "search.writeSpreadsheets('sheets', terms, result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style='margin-top:400px'>SimSight</h1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>created by Mitchell Fanger</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>SimSight is a tool used to perform full text searches on directories of PDF's</h4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Search the public SSED database for a keyword and plot its frequency versus approval date</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Enter keyword:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0936acd401f44bbbd698d582bab36f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thrombogenicity: 67\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "outputText = widgets.Text()\n",
    "inputText = widgets.Text()\n",
    "def sendIn(sender):\n",
    "    search = PSearch('Summaries', 'test1')\n",
    "    results = search.fullTextSearch([inputText.value], False)\n",
    "    search.resultValues([inputText.value], results)\n",
    "    search.getTotalSpreadsheet('Location', results)\n",
    "    search.plotTotalTwo('Location', str(inputText.value))\n",
    "\n",
    "display(HTML('<h1 style=\\'margin-top:400px\\'>SimSight</h1'))\n",
    "display(HTML('<h2>created by Mitchell Fanger</h2>'))\n",
    "display(HTML('<h4>SimSight is a tool used to perform full text searches on directories of PDF\\'s</h4'))\n",
    "display(HTML('<h4>Search the public SSED database for a keyword and plot its frequency versus approval date</h4>'))\n",
    "display(HTML('<h4>Enter keyword:</h4>'))\n",
    "\n",
    "inputText.on_submit(sendIn)\n",
    "\n",
    "inputText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='margin-top:400px'></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 2, 4, 6, 8}\n",
      "{0, 2, 6, 8}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c044e95e4a35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "set1 = {0,1,2,3,4,5,6,7,8,9}\n",
    "set2 = {1,3,5,7,9}\n",
    "\n",
    "for element in set2:\n",
    "    set1.remove(element)\n",
    "    \n",
    "print(set1)\n",
    "set1.remove(4)\n",
    "print(set1)\n",
    "set1.remove(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/16/2015\n",
      "Gastroenterology/Urology\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getKInfo(ID, lines):\n",
    "    url = 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=' + ID\n",
    "    page_response = requests.get(url, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "    first = 'none'\n",
    "    second = 'none'\n",
    "    \n",
    "    textContent = page_content.find_all('tr')\n",
    "    \n",
    "    for i in range(9, len(textContent)):\n",
    "        if 'Decision Date' in str(textContent[i]):\n",
    "            first = str(textContent[i]).split('<td align=\"Left\">')[1][0:10]\n",
    "        elif '510k Review Panel' in str(textContent[i]):\n",
    "            second = str(textContent[i]).split('<td align=\"Left\">')[1].split('</td>')[0]\n",
    "    lines.append([ID, first, second])\n",
    "    return lines\n",
    "\n",
    "def writeToCSV(root, lines, file):\n",
    "    \n",
    "    with open(root + '\\\\' + file + '.csv', 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile)\n",
    "        filewriter.writerows(lines)\n",
    "    \n",
    "def kInfoList(root, phrase, names):\n",
    "    lines = []\n",
    "    \n",
    "    for name in names:\n",
    "        lines = getKInfo(name, lines)\n",
    "        \n",
    "    writeToCSV(root, lines, phrase)\n",
    "ID = 'K131950'\n",
    "url = 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=' + ID\n",
    "page_response = requests.get(url, timeout=5)\n",
    "page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "first = 'none'\n",
    "second = 'none'\n",
    "\n",
    "textContent = page_content.find_all('tr')\n",
    "\n",
    "for i in range(9, len(textContent)):\n",
    "    if 'Decision Date' in str(textContent[i]):\n",
    "        first = str(textContent[i]).split('<td align=\"left\">')[1][1:11]\n",
    "    elif '510k Review Panel' in str(textContent[i]):\n",
    "        second = str(textContent[i]).split('<td align=\"left\">')[1].split('</td>')[0].strip()\n",
    "        \n",
    "print(first.strip())\n",
    "print(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
